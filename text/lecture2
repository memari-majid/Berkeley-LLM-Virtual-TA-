Lecture 2: A Deeper Dive into LLM Agents
Hello, everyone. I’m excited to be here to talk about LLM agents. Today, I will cover three main topics: first, what an LLM agent is; second, a brief history of LLM agents, including the development of LLMs and agents as distinct and converging fields; and third, some future directions for the development of LLM agents. This is a rapidly evolving area, so it's difficult to cover everything in detail, but I will provide an overview of the key concepts and trends shaping the field.
What is an LLM Agent?
To start, let’s define what we mean by an LLM agent. To do this, we must break the term into two parts: "LLM" and "agent." LLMs, or large language models, are models trained to predict text, but what exactly do we mean by "agent"? In AI, "agent" is a broad term that can refer to many types of intelligent systems, from autonomous cars and robots to video game characters and chatbots. Fundamentally, an agent is any intelligent system that interacts with its environment, whether that environment is physical, digital, or even social. For instance, a robot operates in a physical environment, while a chatbot interacts within a digital environment.
The idea of an agent changes over time, especially as the concept of intelligence evolves. Sixty years ago, a simple rule-based chatbot might have been considered intelligent, but today, even advanced models like ChatGPT are no longer surprising to most people.
When we combine LLMs and agents, we get a system where the LLM serves as the reasoning and planning component for the agent, allowing it to interact with its environment in more sophisticated ways. LLM agents can process text inputs and outputs, interact with external tools, retrieve information, and reason through complex tasks.
Three Key Types of Agents
In my view, there are three main categories of agents. First, there are text agents, which interact with their environment through language. These have existed since the early days of AI, with examples like ELIZA, a chatbot from the 1960s that used simple rules to engage users in conversation. Second, we have LLM agents, which are text agents powered by large language models. These agents use the power of LLMs to perform tasks that go beyond basic rule-based systems. Finally, there are reasoning agents, which use LLMs not only to act but also to reason through problems. These agents can plan and solve more complex tasks by combining both reasoning and action.
A Brief History of LLM Agents
LLMs started to gain prominence in 2020 with models like GPT-3. Researchers began exploring their potential across various tasks, including both reasoning tasks (like symbolic question answering) and acting tasks (like playing games or interacting with environments). This led to the development of reasoning agents, which are significantly more capable than earlier agents.
LLM agents have unlocked new methods and applications. For instance, reasoning agents can be used in web interactions, software engineering, scientific discovery, and more. These agents can retrieve information, use external tools, and reason through complex tasks, often in real-time.
Historically, AI has seen different paradigms for building agents. Initially, AI was dominated by symbolic AI, where agents followed explicitly programmed rules. Then came reinforcement learning (RL) agents, particularly deep RL agents, which learn by interacting with their environment and maximizing rewards. Today, we are seeing the emergence of LLM agents, which blend reasoning and acting in ways that previous paradigms could not.
Challenges and Convergence
There are challenges in developing LLM agents. Reasoning, knowledge retrieval, and interaction with tools are often disjointed processes, making it difficult to build a cohesive agent capable of solving a wide range of tasks. For example, while a model might excel at question answering, it might struggle with tasks that require real-world knowledge or complex reasoning, such as calculating the current weather or performing complex computations.
To address these issues, a new approach called ReAct has been proposed, where agents combine reasoning with acting. For example, an agent can think about what it needs to do (reasoning) and then perform an action, such as retrieving data or running a calculation. This synergy between reasoning and acting allows the agent to adjust its actions based on new information, leading to better problem-solving capabilities.
Future Directions for LLM Agents
Looking forward, there are several exciting areas for future research in LLM agents. One important area is long-term memory. Currently, LLMs have limited context windows and struggle to retain information across different tasks or sessions. Developing agents with long-term memory would allow them to build on past experiences and improve their performance over time, much like a human keeps a diary to track important events or learnings.
Another area is digital automation, where agents could handle routine tasks such as writing reports, debugging code, or managing complex workflows. These applications could revolutionize industries like software engineering, scientific research, and customer service.
Finally, we need to improve how we train, interface with, and benchmark these agents. Training models specifically for agent tasks (rather than general language prediction) could lead to more efficient and capable agents. Additionally, designing better interfaces and benchmarks will help ensure that agents are reliable, robust, and able to handle the complexity of real-world tasks.

